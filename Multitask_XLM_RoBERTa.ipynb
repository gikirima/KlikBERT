{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gikirima/KlikBERT/blob/main/Multitask_XLM_RoBERTa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3TWeN3PcbZ5",
        "outputId": "e22ad541-5bc8-45ce-a0f1-50d6165be680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers datasets scikit-learn --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I47vWYpDDMLQ",
        "outputId": "259ad720-e163-48b9-ca72-22b5d4db2047"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dsai_247056015/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2025-06-15 05:12:49.237554: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-06-15 05:12:49.282003: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-06-15 05:12:49.282054: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-06-15 05:12:49.283037: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-15 05:12:49.290195: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwLdBIADDWnV"
      },
      "source": [
        "## âœ… 2. Baca Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO-Kf7VNDce5",
        "outputId": "f37ceebf-04d1-4598-b4d4-7676f74063c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['isi', 'judul', 'url', 'kategori_berita', 'clickbait_label'], dtype='object')\n",
            "['misleading' 'non clickbait' 'teasing' 'exaggeration']\n",
            "['politik' 'kriminal' 'bisnis' 'kesehatan' 'entertainment' 'lingkungan'\n",
            " 'sport' 'teknologi' 'lifestyle']\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = \"https://raw.githubusercontent.com/gikirima/KlikBERT/refs/heads/main/KlikBERT_dataset.csv\"  # ganti sesuai lokasi kamu\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Cek struktur\n",
        "print(df.columns)\n",
        "print(df['clickbait_label'].unique())\n",
        "print(df['kategori_berita'].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "OYby31nzPCGG",
        "outputId": "f950c1c9-102a-4692-e3ac-bec7b484bebc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>judul</th>\n",
              "      <th>isi</th>\n",
              "      <th>clickbait_label</th>\n",
              "      <th>kategori_berita</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4117</td>\n",
              "      <td>4117</td>\n",
              "      <td>4117</td>\n",
              "      <td>4117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>4116</td>\n",
              "      <td>2721</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Atasi Kabut Asap dari Indonesia, Malaysia Renc...</td>\n",
              "      <td>Beauty</td>\n",
              "      <td>misleading</td>\n",
              "      <td>lifestyle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>2</td>\n",
              "      <td>369</td>\n",
              "      <td>1030</td>\n",
              "      <td>822</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    judul     isi  \\\n",
              "count                                                4117    4117   \n",
              "unique                                               4116    2721   \n",
              "top     Atasi Kabut Asap dari Indonesia, Malaysia Renc...  Beauty   \n",
              "freq                                                    2     369   \n",
              "\n",
              "       clickbait_label kategori_berita  \n",
              "count             4117            4117  \n",
              "unique               4               9  \n",
              "top         misleading       lifestyle  \n",
              "freq              1030             822  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# drop columns yang tidak diperlukan\n",
        "df = df[['judul', 'isi','clickbait_label', 'kategori_berita']]\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "df = df.dropna(subset=['judul', 'isi', 'clickbait_label', 'kategori_berita'])\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgwwpYcoPCGH"
      },
      "outputs": [],
      "source": [
        "# drop duplikat\n",
        "df.drop_duplicates(subset=['judul', 'isi'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VICGy4hUPCGI"
      },
      "outputs": [],
      "source": [
        "# drop yang isinya kosong\n",
        "df = df[df['judul'].str.strip() != '']\n",
        "df = df[df['isi'].str.strip() != '']\n",
        "df = df[df['clickbait_label'].str.strip() != '']\n",
        "df = df[df['kategori_berita'].str.strip() != '']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjRb1mHIPCGI",
        "outputId": "c0364983-6a20-49b7-d973-4c88e98b475c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4117, 4)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLihDxkGDrBe"
      },
      "source": [
        "### âœ… 3. Label Encoding (6 Kelas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2Fu1OA9DyPX",
        "outputId": "f256b69f-1b82-4dce-fe4c-241cb2744c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label setelah encoding: [1 2 3 0]\n",
            "Kategori setelah encoding: [6 3 0 2 1 5 7 8 4]\n",
            "Mapping label: {'exaggeration': 0, 'misleading': 1, 'non clickbait': 2, 'teasing': 3}\n",
            "Mapping kategori: {'bisnis': 0, 'entertainment': 1, 'kesehatan': 2, 'kriminal': 3, 'lifestyle': 4, 'lingkungan': 5, 'politik': 6, 'sport': 7, 'teknologi': 8}\n"
          ]
        }
      ],
      "source": [
        "le_clickbait = LabelEncoder()\n",
        "le_kategori = LabelEncoder()\n",
        "df[\"label_encoded\"] = le_clickbait.fit_transform(df[\"clickbait_label\"])\n",
        "df[\"kategori_encoded\"] = le_kategori.fit_transform(df[\"kategori_berita\"])\n",
        "\n",
        "print(\"Label setelah encoding:\", df[\"label_encoded\"].unique())\n",
        "print(\"Kategori setelah encoding:\", df[\"kategori_encoded\"].unique())\n",
        "\n",
        "print(\"Mapping label:\", dict(zip(le_clickbait.classes_, le_clickbait.transform(le_clickbait.classes_))))\n",
        "print(\"Mapping kategori:\", dict(zip(le_kategori.classes_, le_kategori.transform(le_kategori.classes_))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPK39aK5D5-W"
      },
      "source": [
        "### âœ… 4. Split Train/Val/Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThBJZoQrEBOZ"
      },
      "outputs": [],
      "source": [
        "X = df[[\"judul\", \"isi\"]]\n",
        "y = df[[\"label_encoded\", \"kategori_encoded\"]]\n",
        "\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=df[\"label_encoded\"], random_state=42\n",
        ")\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts, temp_labels, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# 2. Ambil masing-masing kolom sebagai list string\n",
        "train_judul = train_texts[\"judul\"].tolist()\n",
        "train_isi   = train_texts[\"isi\"].tolist()\n",
        "val_judul   = val_texts[\"judul\"].tolist()\n",
        "val_isi     = val_texts[\"isi\"].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRTR_nfaEGTY"
      },
      "source": [
        "### âœ… 5. Tokenisasi IndoBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p10NIvdrESJP"
      },
      "outputs": [],
      "source": [
        "# Ganti model ke mBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
        "max_len = 512\n",
        "\n",
        "# 3. Tokenisasi dua input (judul dan isi)\n",
        "train_encodings = tokenizer(\n",
        "    train_judul,\n",
        "    train_isi,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=max_len\n",
        ")\n",
        "val_encodings = tokenizer(\n",
        "    val_judul,\n",
        "    val_isi,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=max_len\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKvWuiH0PCGM"
      },
      "outputs": [],
      "source": [
        "# bagi y menjadi dua label terpisah\n",
        "train_clickbait_labels = train_labels[\"label_encoded\"].tolist()\n",
        "train_kategori_labels  = train_labels[\"kategori_encoded\"].tolist()\n",
        "\n",
        "val_clickbait_labels = val_labels[\"label_encoded\"].tolist()\n",
        "val_kategori_labels  = val_labels[\"kategori_encoded\"].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAMJfDg7EWNr"
      },
      "source": [
        "### âœ… 6. PyTorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5T6EO7KxEdDl"
      },
      "outputs": [],
      "source": [
        "class ClickbaitDataset(Dataset):\n",
        "    def __init__(self, encodings, clickbait_labels, kategori_labels):\n",
        "        self.encodings = encodings\n",
        "        self.clickbait_labels = clickbait_labels\n",
        "        self.kategori_labels = kategori_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clickbait_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"clickbait_labels\"] = torch.tensor(self.clickbait_labels[idx], dtype=torch.long)\n",
        "        item[\"kategori_labels\"] = torch.tensor(self.kategori_labels[idx], dtype=torch.long)\n",
        "\n",
        "        return item\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = ClickbaitDataset(train_encodings, train_clickbait_labels, train_kategori_labels)\n",
        "val_dataset   = ClickbaitDataset(val_encodings, val_clickbait_labels, val_kategori_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9_QRrefEgpD"
      },
      "source": [
        "### âœ… 7. Bangun Model IndoBERT Kustom Multitask-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u23Iw07aEla3"
      },
      "outputs": [],
      "source": [
        "class IndoBERTClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_clickbait_labels, num_kategori_labels):\n",
        "        super(IndoBERTClassifier, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "\n",
        "        self.num_clickbait_labels = num_clickbait_labels\n",
        "        self.num_kategori_labels = num_kategori_labels\n",
        "\n",
        "        self.clickbait_classifier = nn.Linear(hidden_size, num_clickbait_labels)\n",
        "        self.kategori_classifier = nn.Linear(hidden_size, num_kategori_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, clickbait_labels=None, kategori_labels=None):\n",
        "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = output.last_hidden_state[:, 0, :]  # [CLS]\n",
        "\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        clickbait_logits = self.clickbait_classifier(dropout_output)\n",
        "        kategori_logits = self.kategori_classifier(dropout_output)\n",
        "\n",
        "        loss = None\n",
        "        loss_clickbait = None\n",
        "        loss_kategori = None\n",
        "        if clickbait_labels is not None and kategori_labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss_clickbait = loss_fct(clickbait_logits.view(-1, self.num_clickbait_labels), clickbait_labels.view(-1))\n",
        "            loss_kategori = loss_fct(kategori_logits.view(-1, self.num_kategori_labels), kategori_labels.view(-1))\n",
        "            loss = loss_clickbait + loss_kategori # total loss bisa ditambahkan atau dibagi 2\n",
        "        return {\"loss\": loss,\n",
        "                \"loss_clickbait\": loss_clickbait,\n",
        "                \"loss_kategori\": loss_kategori,\n",
        "                \"clickbait_logits\": clickbait_logits,\n",
        "                \"kategori_logits\": kategori_logits}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWem-PLAEo_0"
      },
      "source": [
        "### âœ… 8. Trainer Setup dan Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9fK0KIaEvWa"
      },
      "outputs": [],
      "source": [
        "model = IndoBERTClassifier(\n",
        "    \"FacebookAI/xlm-roberta-base\",\n",
        "    num_clickbait_labels=len(le_clickbait.classes_),\n",
        "    num_kategori_labels=len(le_kategori.classes_)\n",
        "    )\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/KlikBERT/XLM-RoBERTa/results\",\n",
        "    num_train_epochs=10, # hyperparameter ini bisa disesuaikan\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_steps=21,\n",
        "    save_steps=21,\n",
        "    learning_rate=2e-5, # learning rate yang umum digunakan, hyperparameter ini bisa disesuaikan\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=21,\n",
        "    logging_strategy=\"steps\",\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        "    metric_for_best_model=\"eval_loss\", # Tentukan metrik untuk model terbaik\n",
        "    load_best_model_at_end=True,     # Muat model terbaik di akhir training\n",
        "    greater_is_better=False          # Karena metriknya loss, nilai lebih kecil lebih baik\n",
        ")\n",
        "\n",
        "# === Custom collate function ===\n",
        "def custom_collate_fn(batch):\n",
        "    return {\n",
        "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n",
        "        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n",
        "        \"clickbait_labels\": torch.tensor([item[\"clickbait_labels\"] for item in batch]),\n",
        "        \"kategori_labels\": torch.tensor([item[\"kategori_labels\"] for item in batch]),\n",
        "\n",
        "    }\n",
        "\n",
        "# Custom Trainer untuk multi-task\n",
        "class MultiTaskTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self._last_logged_step = -1  # tracker untuk mencegah duplikasi lo\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        clickbait_labels = inputs.pop(\"clickbait_labels\")\n",
        "        kategori_labels  = inputs.pop(\"kategori_labels\")\n",
        "        outputs = model(**inputs, clickbait_labels=clickbait_labels, kategori_labels=kategori_labels)\n",
        "\n",
        "        loss = outputs[\"loss\"]\n",
        "\n",
        "        current_step = self.state.global_step\n",
        "        if self.is_in_train and current_step != self._last_logged_step:\n",
        "            self._last_logged_step = current_step  # update tracker\n",
        "            self.log({\n",
        "                \"loss_clickbait\": outputs[\"loss_clickbait\"].mean().item(),\n",
        "                \"loss_kategori\": outputs[\"loss_kategori\"].mean().item()\n",
        "            })\n",
        "\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "\n",
        "trainer = MultiTaskTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=custom_collate_fn,\n",
        "    # resume_from_checkpoint=\"./results/checkpoint-XXXX\" # uncomment untuk resume training, ganti XXXX dengan step checkpoint terakhir\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4quqnWPnFpZN"
      },
      "source": [
        "### âœ… 9. Jalankan Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "nNvnHxVQFww-",
        "outputId": "3b83c6fb-03a8-4b30-86b1-d2ba9079b8bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='901' max='910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [901/910 17:45 < 00:10, 0.84 it/s, Epoch 9.89/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>3.099500</td>\n",
              "      <td>2.699332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>2.514400</td>\n",
              "      <td>2.179503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>2.130600</td>\n",
              "      <td>1.771752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.795100</td>\n",
              "      <td>1.580124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>1.628900</td>\n",
              "      <td>1.501491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>1.417000</td>\n",
              "      <td>1.421861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>1.344600</td>\n",
              "      <td>1.316680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>1.486600</td>\n",
              "      <td>1.320257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>1.099000</td>\n",
              "      <td>1.301497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.134300</td>\n",
              "      <td>1.207493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>1.110200</td>\n",
              "      <td>1.201129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>1.040500</td>\n",
              "      <td>1.190760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>273</td>\n",
              "      <td>1.074100</td>\n",
              "      <td>1.190890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>294</td>\n",
              "      <td>0.859500</td>\n",
              "      <td>1.169114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>0.841200</td>\n",
              "      <td>1.179739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.837400</td>\n",
              "      <td>1.153137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>357</td>\n",
              "      <td>0.788100</td>\n",
              "      <td>1.188754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>378</td>\n",
              "      <td>0.734100</td>\n",
              "      <td>1.201353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>0.657900</td>\n",
              "      <td>1.190573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.695200</td>\n",
              "      <td>1.177395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>441</td>\n",
              "      <td>0.630300</td>\n",
              "      <td>1.192415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>462</td>\n",
              "      <td>0.680900</td>\n",
              "      <td>1.190132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>483</td>\n",
              "      <td>0.519700</td>\n",
              "      <td>1.201903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>504</td>\n",
              "      <td>0.553200</td>\n",
              "      <td>1.214283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>525</td>\n",
              "      <td>0.523200</td>\n",
              "      <td>1.197782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>546</td>\n",
              "      <td>0.515500</td>\n",
              "      <td>1.188931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>567</td>\n",
              "      <td>0.408000</td>\n",
              "      <td>1.222339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>588</td>\n",
              "      <td>0.426300</td>\n",
              "      <td>1.218231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>609</td>\n",
              "      <td>0.379100</td>\n",
              "      <td>1.303002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.414000</td>\n",
              "      <td>1.213589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>651</td>\n",
              "      <td>0.351800</td>\n",
              "      <td>1.248898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>672</td>\n",
              "      <td>0.292400</td>\n",
              "      <td>1.284317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>693</td>\n",
              "      <td>0.335300</td>\n",
              "      <td>1.253213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>714</td>\n",
              "      <td>0.285900</td>\n",
              "      <td>1.263508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>735</td>\n",
              "      <td>0.272100</td>\n",
              "      <td>1.266601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>756</td>\n",
              "      <td>0.243400</td>\n",
              "      <td>1.306603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>777</td>\n",
              "      <td>0.264300</td>\n",
              "      <td>1.320441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>798</td>\n",
              "      <td>0.243900</td>\n",
              "      <td>1.286929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>819</td>\n",
              "      <td>0.213200</td>\n",
              "      <td>1.296121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>0.212300</td>\n",
              "      <td>1.307619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>861</td>\n",
              "      <td>0.206300</td>\n",
              "      <td>1.307410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>882</td>\n",
              "      <td>0.216700</td>\n",
              "      <td>1.303262</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKzrGkNyPCGO"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ambil log history dari trainer\n",
        "log_hist = trainer.state.log_history\n",
        "\n",
        "# Konversi ke DataFrame\n",
        "df_log = pd.DataFrame(trainer.state.log_history)\n",
        "\n",
        "# Gunakan epoch daripada step\n",
        "df_loss = df_log[df_log['epoch'].notna()][['epoch', 'loss', 'eval_loss']]\n",
        "df_loss = df_loss.rename(columns={'epoch': 'Epoch'})\n",
        "\n",
        "# Ubah ke format long agar bisa pakai sns.lineplot\n",
        "df_melted = df_loss.melt(id_vars='Epoch',\n",
        "                         value_vars=['loss', 'eval_loss'],\n",
        "                         var_name='Type',\n",
        "                         value_name='Loss')\n",
        "\n",
        "# Rename label agar rapi\n",
        "df_melted['Type'] = df_melted['Type'].replace({\n",
        "    'loss': 'Training Loss',\n",
        "    'eval_loss': 'Validation Loss'\n",
        "})\n",
        "\n",
        "# Plot pakai seaborn\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.lineplot(data=df_melted, x='Epoch', y='Loss', hue='Type', marker='o')\n",
        "plt.title('Training vs Validation Loss (per Epoch)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW6a689ePCGP"
      },
      "outputs": [],
      "source": [
        "# --- KODE PERSIAPAN DATA BARU YANG LEBIH BAIK ---\n",
        "\n",
        "# 1. Pilih hanya kolom yang kita perlukan\n",
        "df_relevant = df_log[['step', 'loss', 'loss_clickbait', 'loss_kategori']].copy()\n",
        "\n",
        "# 2. Kelompokkan berdasarkan 'step'. Untuk setiap step, ambil nilai non-kosong pertama\n",
        "#    yang ditemukan untuk setiap jenis loss. Ini akan menggabungkan beberapa baris log\n",
        "#    untuk step yang sama menjadi satu baris tunggal.\n",
        "df_agg = df_relevant.groupby('step').agg({\n",
        "    'loss': 'first',\n",
        "    'loss_clickbait': 'first',\n",
        "    'loss_kategori': 'first'\n",
        "}).reset_index()\n",
        "\n",
        "# 3. Buang baris yang datanya tidak lengkap (misalnya, log evaluasi yang tidak punya 'loss')\n",
        "df_train_loss = df_agg.dropna()\n",
        "\n",
        "\n",
        "# --- Sisa kode plotting tetap sama (tidak perlu diubah) ---\n",
        "\n",
        "if df_train_loss.empty:\n",
        "    print(\"Tidak ada data training loss yang bisa diplot. Pastikan training sudah berjalan.\")\n",
        "else:\n",
        "    # Ubah nama kolom agar lebih rapi di legenda grafik\n",
        "    df_train_loss.rename(columns={\n",
        "        'loss': 'Loss Gabungan',\n",
        "        'loss_clickbait': 'Loss Clickbait',\n",
        "        'loss_kategori': 'Loss Kategori'\n",
        "    }, inplace=True)\n",
        "\n",
        "    # \"Melt\" dataframe agar bisa diplot dengan mudah oleh Seaborn\n",
        "    df_melted = df_train_loss.melt(\n",
        "        id_vars='step',\n",
        "        value_vars=['Loss Gabungan', 'Loss Clickbait', 'Loss Kategori'],\n",
        "        var_name='Tipe Loss',\n",
        "        value_name='Nilai Loss'\n",
        "    )\n",
        "\n",
        "    # Buat plot (tanpa confidence interval)\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    sns.lineplot(data=df_melted, x='step', y='Nilai Loss', hue='Tipe Loss', errorbar=None)\n",
        "    plt.title('Grafik Training Loss (Gabungan, Clickbait, & Kategori)')\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PS68IYaVPCGP"
      },
      "outputs": [],
      "source": [
        "df_log.to_csv('/content/drive/KlikBERT/XLM-RoBERTa/history_log.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkpiQ9KPF3sc"
      },
      "source": [
        "### âœ… A. Evaluasi Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFO9xlhXGJtO"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# 1. Dapatkan prediksi dari data validasi\n",
        "preds_output = trainer.predict(val_dataset)\n",
        "\n",
        "# --- PERBAIKAN ---\n",
        "# Model mengembalikan tuple berisi (loss_clickbait, loss_kategori, clickbait_logits, kategori_logits)\n",
        "# Kita ambil logits dari indeks 2 dan 3.\n",
        "all_preds_outputs = preds_output.predictions\n",
        "pred_clickbait_logits = all_preds_outputs[2]\n",
        "pred_kategori_logits  = all_preds_outputs[3]\n",
        "# --- AKHIR PERBAIKAN ---\n",
        "\n",
        "# Ambil prediksi masing-masing task:\n",
        "pred_clickbait = torch.argmax(torch.tensor(pred_clickbait_logits), dim=1)\n",
        "pred_kategori  = torch.argmax(torch.tensor(pred_kategori_logits), dim=1)\n",
        "\n",
        "# Label ground-truth\n",
        "labels_clickbait = torch.tensor([x[\"clickbait_labels\"] for x in val_dataset])\n",
        "labels_kategori  = torch.tensor([x[\"kategori_labels\"] for x in val_dataset])\n",
        "\n",
        "# Pisahkan logits\n",
        "logits_clickbait = preds_output.predictions[0]\n",
        "logits_kategori = preds_output.predictions[1]\n",
        "\n",
        "all_kategori_labels = np.arange(len(le_kategori.classes_))\n",
        "\n",
        "# 3. Evaluasi\n",
        "print(\"âœ… Classification Report:\")\n",
        "print(classification_report(labels_clickbait, pred_clickbait, target_names=le_clickbait.classes_, zero_division=0))\n",
        "print(classification_report(labels_kategori, pred_kategori, labels=all_kategori_labels, target_names=le_kategori.classes_, zero_division=0))\n",
        "\n",
        "# Evaluasi Kategori\n",
        "print(\"ðŸ“Œ [Kategori]\")\n",
        "print(\"Accuracy:\", accuracy_score(labels_kategori, pred_kategori))\n",
        "# Evaluasi Clickbait\n",
        "print(\"ðŸ“Œ [Clickbait]\")\n",
        "print(\"Accuracy:\", accuracy_score(labels_clickbait, pred_clickbait))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEclodMjHdUo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Confusion matrix untuk Clickbait\n",
        "cm_clickbait = confusion_matrix(labels_clickbait, pred_clickbait)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm_clickbait, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=le_clickbait.classes_,\n",
        "            yticklabels=le_clickbait.classes_)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix â€“ Clickbait\")\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix untuk Kategori\n",
        "cm_kategori = confusion_matrix(labels_kategori, pred_kategori)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_kategori, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
        "            xticklabels=le_kategori.classes_,\n",
        "            yticklabels=le_kategori.classes_)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix â€“ Kategori\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugd1Zu80IQVO"
      },
      "source": [
        "### âœ… B. Simpan Model (format .bin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIYULwbyIRjC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "save_path = \"/content/drive/KlikBERT/XLM-RoBERTa/model/\"\n",
        "os.makedirs(save_path, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSUpIfPTIcMn"
      },
      "source": [
        "simpan model dan tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThRt-yL2IeMm"
      },
      "outputs": [],
      "source": [
        "# Simpan model\n",
        "torch.save(model.state_dict(), os.path.join(save_path, \"pytorch_model.bin\"))\n",
        "\n",
        "# Simpan tokenizer\n",
        "tokenizer.save_pretrained(save_path)\n",
        "model.bert.config.save_pretrained(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J43sNBD3cbaJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}